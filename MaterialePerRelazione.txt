Perchè abbiamo usato pandas per l'analisi dei dataset:
https://www.geekandjob.com/wiki/pandas

spiegazione della nostra classe VectorFeatures:
vedendo il codice di NN, la prof aveva definito una classe Sample che gestiva 
i campioni e poi nel classificatore ne crea una lista. Noi per gestire i 
nostri campioni, abbiamo già la classe VectorFeatures che per il momento 
la utilizziamo solo per scrivere sul csv. Utilizzando l'overload dei costruttori,
è possibile definire un ulteriore costruttore a cui si passa la "linea" 
che il classificatore legge dal training set. Con la funzione split si separano 
tutte le features e le inserisco in un vettore di stringhe. Solo dopo con 
un for memorizzo tutti gli elementi dell'array di stringhe nel nostro array di double 
features, utilizando il parseDouble. L'ultimo elemento dell'array di stringhe, lo 
lascio fuori e lo memorizzo nella nostra actionKey con la parseInt (la nostra actionKey, 
in questo contesto, altro non è che la classe a cui il campione appartiene). 
Ho inoltre scritto il metodo per calcolare la distanza tra due campioni 
utilizzando tutte le features.

perchè NN e perchè non KNN
KNN -> 
al di la della classe che si occupa proprio di classificare i punti, e quindi 
del classificatore in se, dietro di questa c'è una classe che costruisce 
un albero binario ordinato per tutta la lista di campioni che rappresenta il 
training set. poi per trovare i k vicini più vicini fa una coda con priorità 
sulla quale utilizza la ricorsione e calcola le distanze in un modo che non 
mi è per niente chiaro. 
NN-> 
legge il csv e salva i campioni in una lista utilizzando il costruttore di 
VectorFeatures(String line) e poi per il calcolo della distanza prende il 
primo campione della lista e calcola la distanza con il campione "attuale". 
questa prima distanza sarà la distanza minima e questo primo campione sarà 
il campione più vicino. poi in un for-each, scorre tutta la lista e per ogni 
elemento della lista calcola la distanza con il campione attuale. se trova 
una distanza più piccola della distanza minima, aggiorna la distanza minima 
e il punto e ritorna il punto stesso (nel nostro caso dovrebbe ritornare la 
actionKey del punto più vicino e in base alla actionKey, 
corrisponde l'azione. in pratica come abbiamo fatto stamattina)

Per quanto riguarda la scelta tra algoritmi in realtà seppur il K-NN abbia performance migliori rispetto all'NN, il nostro problema è la quantità di dati che abbiamo
essendo che il knn calcola tutto al momento della predizione
lui ogni volta dovrà memorizzare tutti i sample e scorrerseli
è visto che ogni guida so 20.000 righe o giù per lì
knn è adatto per un training set limitato
NN nel nostro caso è adatto perchè penso che la sua imprecisione venga compensata dal fatto che abbiamo tantissimi sample

https://www.numberanalytics.com/blog/comprehensive-min-max-scaling-guide

[23:09, 07/06/2025] Michela Uni: Per quanto riguarda la normalizzazione, lo Z score ha come obiettivo quello di prendere i dati di una determinata categoria (ex. velocità) e assimilarli ad una gaussiana simmetrica, in questo modo ciascuna categoria di dati è comparabile nonostante faccia riferimento a grandezze e scale diverse (il perché è una roba strana matematica sulle gaussiane)
[23:10, 07/06/2025] Michela Uni: il min-max scaling (che è quello di cui avevamo parlato stamattina) ha come obiettivo quello di rendere ciascuna grandezza compresa in un intervallo tra [0-1], in questo modo diventa più semplice calcolare distanza e tutto

per quanto riguarda il minimo e il massimo non dobbiamo prendere quelli del training set perché altrimenti staremo effettuando un overfitting (il nostro algoritmo si baserebbe troppo sul dataset e quando posto in una situazione che genera un vettore di features lontano non risponderebbe correttamente)
